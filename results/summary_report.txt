--- MULTI-AGENT EXPERIMENT SUMMARY ---
Output directory: results

=== Experiment: sensor_test_100ep ===
  Runs: 1
  Episodes per run: 100
  Parameters: {'num_agents': 5, 'num_neurons': 50, 'vector_dim': 16, 'connectivity': 0.15, 'max_steps_per_episode': 500, 'grid_size': 15, 'initial_exploration': 1.0, 'exploration_decay': 0.995, 'social_learning_freq': 25, 'elite_ratio': 0.2, 'learner_ratio': 0.5, 'synapses_per_transfer': 10, 'revolution_threshold': 0.5, 'revolution_window': 10, 'revolution_elite_ratio': 0.1, 'environment_config': {'grid_size': 15}}

  Total Episodes: 100
  Final Avg Reward: -176.1000
  Best Reward Achieved: 0.0000

  Social Learning:
    Total Transfers: 0
    Total Synapses: 0

  Revolution Protocol:
    Total Revolutions: 0

  Final Phase (last 10% episodes):
    Avg Reward: -175.3880

  Learning Trend (every 10%):
    Episodes 0-10: Avg Reward = -175.2353
    Episodes 10-20: Avg Reward = -174.6680
    Episodes 20-30: Avg Reward = -174.5960
    Episodes 30-40: Avg Reward = -174.1480
    Episodes 40-50: Avg Reward = -174.9960
    Episodes 50-60: Avg Reward = -174.2280
    Episodes 60-70: Avg Reward = -174.5400
    Episodes 70-80: Avg Reward = -174.2200
    Episodes 80-90: Avg Reward = -174.4040
    Episodes 90-100: Avg Reward = -175.3880

