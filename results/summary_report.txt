--- MULTI-AGENT EXPERIMENT SUMMARY ---
Output directory: results

=== Experiment: sensor_test_10ep ===
  Runs: 1
  Episodes per run: 10
  Parameters: {'num_agents': 5, 'num_neurons': 50, 'vector_dim': 16, 'connectivity': 0.15, 'max_steps_per_episode': 100, 'grid_size': 10, 'initial_exploration': 1.0, 'exploration_decay': 0.995, 'social_learning_freq': 5, 'elite_ratio': 0.2, 'learner_ratio': 0.5, 'synapses_per_transfer': 10, 'revolution_threshold': 0.5, 'revolution_window': 5, 'revolution_elite_ratio': 0.1, 'environment_config': {'grid_size': 10}}

  Total Episodes: 10
  Final Avg Reward: 0.0000
  Best Reward Achieved: 0.0000

  Social Learning:
    Total Transfers: 0
    Total Synapses: 0

  Revolution Protocol:
    Total Revolutions: 0

  Final Phase (last 10% episodes):
    Avg Reward: 0.0000

  Learning Trend (every 10%):
    Episodes 0-1: Avg Reward = 0.0000
    Episodes 1-2: Avg Reward = 0.0000
    Episodes 2-3: Avg Reward = 0.0000
    Episodes 3-4: Avg Reward = 0.0000
    Episodes 4-5: Avg Reward = 0.0000
    Episodes 5-6: Avg Reward = 0.0000
    Episodes 6-7: Avg Reward = 0.0000
    Episodes 7-8: Avg Reward = 0.0000
    Episodes 8-9: Avg Reward = 0.0000
    Episodes 9-10: Avg Reward = 0.0000

